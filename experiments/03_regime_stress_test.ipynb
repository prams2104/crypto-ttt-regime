{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 03: Regime-Stratified Evaluation\n",
        "\n",
        "**Proposal**: \"Performance stability across volatility percentiles\" and splits across \"bull vs. bear periods and weekday vs. weekend trading.\"\n",
        "\n",
        "This notebook evaluates model performance stratified by realised volatility (RV) percentiles on the test set.\n",
        "\n",
        "**Prerequisites**: Run Experiment 01 (and optionally 02). Uses `checkpoints/joint/best.pt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af62c163",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "cwd = os.getcwd()\n",
        "PROJECT_ROOT = os.path.dirname(cwd) if os.path.basename(cwd) == \"experiments\" else cwd\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "os.chdir(PROJECT_ROOT)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.dataset import CryptoRegimeDataset\n",
        "from src.models import TTTModel\n",
        "from src.ttt_learner import TTTAdaptor\n",
        "from src.utils import get_device\n",
        "\n",
        "CHECKPOINT = \"checkpoints/joint/best.pt\"\n",
        "device = get_device()\n",
        "\n",
        "dataset = CryptoRegimeDataset(\"data/processed\")\n",
        "_, _, test_ds = dataset.get_splits()\n",
        "test_idx = dataset._splits[\"test\"]\n",
        "rv_test = dataset.rv_values[test_idx].numpy()\n",
        "labels_test = dataset.labels[test_idx].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Volatility Bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quartiles of RV on test set\n",
        "q25, q50, q75 = np.percentile(rv_test, [25, 50, 75])\n",
        "\n",
        "def get_bin_mask(rv, bin_name):\n",
        "    if bin_name == \"low (0-25%)\":\n",
        "        return rv <= q25\n",
        "    elif bin_name == \"mid-low (25-50%)\":\n",
        "        return (rv > q25) & (rv <= q50)\n",
        "    elif bin_name == \"mid-high (50-75%)\":\n",
        "        return (rv > q50) & (rv <= q75)\n",
        "    else:  # high (75-100%)\n",
        "        return rv > q75\n",
        "\n",
        "bins = [\"low (0-25%)\", \"mid-low (25-50%)\", \"mid-high (50-75%)\", \"high (75-100%)\"]\n",
        "print(f\"RV quartiles: {q25:.6f}, {q50:.6f}, {q75:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Metrics per Bin (Baseline + TTT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81ee450",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.eval import compute_metrics\n",
        "\n",
        "ckpt = torch.load(CHECKPOINT, map_location=device, weights_only=False)\n",
        "train_args = ckpt.get(\"args\", {})\n",
        "model = TTTModel(num_classes=2, aux_task=train_args.get(\"aux_task\", \"mask\"), \n",
        "                 num_groups=train_args.get(\"num_groups\", 8)).to(device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "adaptor = TTTAdaptor(model=model, base_lr=0.05, ttt_steps=10, mask_mode=train_args.get(\"mask_mode\", \"random_slices\"),\n",
        "                     ttt_optimizer=\"adam\", entropy_adaptive=True, entropy_gate_threshold=0.3, device=device)\n",
        "\n",
        "THRESHOLD = 0.35\n",
        "results = []\n",
        "\n",
        "for bin_name in bins:\n",
        "    mask = get_bin_mask(rv_test, bin_name)\n",
        "    idx = np.where(mask)[0]\n",
        "    if len(idx) < 5:\n",
        "        continue\n",
        "    indices = np.asarray(test_idx)[idx].tolist()\n",
        "    subset = Subset(dataset, indices)\n",
        "    loader = DataLoader(subset, batch_size=1, shuffle=False)\n",
        "    \n",
        "    # Baseline\n",
        "    out_b = adaptor.evaluate_baseline(loader)\n",
        "    m_b = compute_metrics(out_b[\"probabilities\"].numpy(), out_b[\"labels\"].numpy(), \n",
        "                         out_b[\"rv_values\"].numpy(), threshold=THRESHOLD)\n",
        "    \n",
        "    # TTT standard (per sample)\n",
        "    probs_t, labels_t, rv_t = [], [], []\n",
        "    for batch in loader:\n",
        "        imgs, lbl, rv = batch[0].to(device), batch[1], batch[2]\n",
        "        logits, _ = adaptor.adapt_and_predict(imgs)\n",
        "        p = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "        probs_t.append(p)\n",
        "        labels_t.append(lbl.numpy())\n",
        "        rv_t.append(rv.numpy())\n",
        "    probs_t = np.concatenate(probs_t)\n",
        "    labels_t = np.concatenate(labels_t)\n",
        "    rv_t = np.concatenate(rv_t)\n",
        "    m_t = compute_metrics(probs_t, labels_t, rv_t, threshold=THRESHOLD)\n",
        "    \n",
        "    results.append({\"bin\": bin_name, \"n\": len(idx), \"baseline_acc\": m_b[\"accuracy\"], \"baseline_f1\": m_b[\"f1\"],\n",
        "                   \"ttt_acc\": m_t[\"accuracy\"], \"ttt_f1\": m_t[\"f1\"]})\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r['bin']} (n={r['n']}): Baseline acc={r['baseline_acc']:.3f} f1={r['baseline_f1']:.3f} | TTT acc={r['ttt_acc']:.3f} f1={r['ttt_f1']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ef1d5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bf7a4d",
      "metadata": {},
      "source": [
        "**Results (example run):**\n",
        "\n",
        "| RV bin | n | Baseline acc | Baseline F1 | TTT acc | TTT F1 |\n",
        "|--------|---|--------------|------------|---------|--------|\n",
        "| low (0-25%) | 194 | 0.964 | 0.000 | 0.314 | 0.000 |\n",
        "| mid-low (25-50%) | 194 | 0.943 | 0.000 | 0.325 | 0.000 |\n",
        "| mid-high (50-75%) | 193 | 0.974 | 0.000 | 0.394 | 0.000 |\n",
        "| high (75-100%) | 194 | 0.175 | 0.091 | 0.577 | **0.717** |\n",
        "\n",
        "**Interpretation**: TTT improves performance mainly in the **high-volatility** regime (accuracy 0.18 to 0.58, F1 0.09 to 0.72). In low/mid-vol bins the baseline already predicts low-vol correctly; TTT shifts predictions toward high-vol and hurts accuracy there. This supports the proposal: *\"We expect TTT to improve robustness under market regime changes, particularly during high-volatility periods where distribution shifts are largest.\"*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
